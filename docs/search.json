[
  {
    "objectID": "blog-post.html",
    "href": "blog-post.html",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "",
    "text": "Machine Learning Simplified\n\nWhy machine learning jargon feels complex.\nThe importance of understanding these terms for beginners.\n\nExplaining Machine Learning Jargon\n\nKey terms explained in simple language:\nRidge Regression\nDummy Regressor\nStandardScaler\nGrid Search\nCross-Validation\nMean Square Error (MSE)\nRoot Mean Square Error (RMSE)\nMean Absolute Error (MAE)\n\nConclusion\n\nRecap key points and encourage further exploration."
  },
  {
    "objectID": "blog-post.html#ridge-regression",
    "href": "blog-post.html#ridge-regression",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "1. Ridge Regression",
    "text": "1. Ridge Regression\nRidge Regression is a method used to predict outcomes while preventing the model from becoming overly complicated. It works by adding a small penalty to the model’s coefficients, keeping them from becoming too large and ensuring the model stays balanced.\nThis method helps avoid overfitting (memorizing the data), where the model performs well on the training data but struggles with new, unseen data. By keeping the model simple, Ridge Regression ensures it makes reliable predictions in different situations.\nImagine you’re predicting student math grades. Without Ridge Regression, a factor like “studytime” might completely overshadow others like “failures” or “age,” making the model less reliable. Ridge Regression keeps all factors in check, ensuring they work together for better and more accurate predictions."
  },
  {
    "objectID": "blog-post.html#dummy-regressor",
    "href": "blog-post.html#dummy-regressor",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "2. Dummy Regressor",
    "text": "2. Dummy Regressor\nThe Dummy Regressor is a very simple model that predicts the average value of the target variable without analyzing any patterns in the data.\nIt provides a baseline for comparison. By starting with this simple approach, you can determine whether more advanced models are actually improving predictions or just adding unnecessary complexity.\nLet’s say the average math grade for a class is 10.3. A Dummy Regressor will predict 10.3 for every student, regardless of their age, past failures, or time spent studying. While it is not a “smart” model, it gives you a benchmark to evaluate the performance of more advanced methods."
  },
  {
    "objectID": "blog-post.html#standardscaler",
    "href": "blog-post.html#standardscaler",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "3. StandardScaler",
    "text": "3. StandardScaler\nStandardScaler is a tool that adjusts numerical features so they have a similar range, typically with a mean of 0 and a standard deviation of 1.\nIn machine learning, features with larger values can disproportionately influence the model’s predictions. StandardScaler ensures that all features are treated equally, preventing any single factor (like age) from dominating the results.\nImagine predicting student grades using “age” (which ranges from 15 to 22) and “studytime” (which ranges from 1 to 4). Without scaling, the model might give more importance to age simply because its values are larger. StandardScaler solves this by standardizing both features to the same scale, allowing the model to consider them fairly."
  },
  {
    "objectID": "blog-post.html#grid-search",
    "href": "blog-post.html#grid-search",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "4. Grid Search",
    "text": "4. Grid Search\nGrid Search is a method used to systematically test different settings (called hyperparameters) for your model to find the combination that works best. It is like trying out various configurations to optimize the model’s performance.\nMachine learning models often rely on hyperparameters and choosing the right hyperparameters is crucial for improving the accuracy and reliability of predictions, and Grid Search helps automate this process.\nIn Ridge Regression, the hyperparameter (called alpha) controls the balance between simplicity and accuracy. Grid Search tests multiple values of this hyperparamter to see which one results in the best predictions. This ensures the model is tuned for optimal performance, saving time and effort compared to manual trial and error."
  },
  {
    "objectID": "blog-post.html#cross-validation",
    "href": "blog-post.html#cross-validation",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "5. Cross-Validation",
    "text": "5. Cross-Validation\nCross-Validation is a method used to evaluate a model’s performance by testing it on different subsets of the data. It involves splitting the data into smaller parts, training the model on some parts, and testing it on the others.\nThis technique ensures that the model isn’t just performing well on the specific data it was trained on, but can also generalize to new, unseen data. It helps detect overfitting, where a model memorizes the training data but fails on other data.\nImagine dividing the student data into 5 equal parts. In the first round, train the model on 4 parts and test it on the 5th. In the second round, train on a different set of 4 parts and test on another. Repeat this process until every part has been tested once. By averaging the results, you get a more reliable measure of the model’s performance."
  },
  {
    "objectID": "blog-post.html#mean-square-error-mse",
    "href": "blog-post.html#mean-square-error-mse",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "6. Mean Square Error (MSE)",
    "text": "6. Mean Square Error (MSE)\nThe Mean Squared Error (MSE) is a common metric used to evaluate how well a model predicts outcomes. It works by calculating the average of the squared differences between the predicted values and the actual values. By squaring these differences, the MSE gives more weight to larger errors, ensuring significant mistakes have a bigger impact on the overall score.\nIn simple terms, the MSE tells us how far off our predictions are, on average. A smaller MSE indicates better accuracy, meaning the model’s predictions are closer to the actual values. This makes MSE a reliable way to measure the performance of predictive models."
  },
  {
    "objectID": "blog-post.html#root-mean-square-error-rmse",
    "href": "blog-post.html#root-mean-square-error-rmse",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "7. Root Mean Square Error (RMSE)",
    "text": "7. Root Mean Square Error (RMSE)\nThe Root Mean Squared Error (RMSE) is derived by taking the square root of the Mean Squared Error (MSE). This transformation converts the error from squared units back to the original scale of the outcome, making it easier to understand and interpret.\nRMSE represents the average difference between the predicted and actual values, expressed in the same units as the outcome. For example, if RMSE is 4.0, it means that, on average, the model’s predictions are off by 4.0 points. This makes RMSE an intuitive and practical way to assess the accuracy of predictions."
  },
  {
    "objectID": "blog-post.html#mean-absolute-error-mae",
    "href": "blog-post.html#mean-absolute-error-mae",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "8. Mean Absolute Error (MAE)",
    "text": "8. Mean Absolute Error (MAE)\nThe Mean Absolute Error (MAE) is a metric that calculates the average of the absolute differences between the predicted and actual values. Unlike the Mean Squared Error, MAE does not square the differences, making it a simpler and more direct measure of prediction accuracy.\nMAE provides a clear interpretation of how far off, on average, the predictions are from the actual values. For example, an MAE of 3.0 means the model’s predictions differ from the actual outcomes by 3.0 points on average. The MAE a straightforward way assess a model’s performance."
  },
  {
    "objectID": "blog-post.html#step-1-load-and-preprocess-the-data",
    "href": "blog-post.html#step-1-load-and-preprocess-the-data",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "Step 1: Load and Preprocess the Data",
    "text": "Step 1: Load and Preprocess the Data\nWe load the dataset, split it into training and testing sets, and scale the features to ensure that all numerical variables (like age and study time) are on the same range.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Load data\nstudent_df = pd.read_csv(\"data/student.csv\")\n\n# Split data\ntrain_df, test_df = train_test_split(student_df, test_size=0.3, random_state=123)\n\n# Define features and target\nX_train = train_df[[\"age\", \"studytime\", \"failures\"]]\ny_train = train_df[\"G3\"]\nX_test = test_df[[\"age\", \"studytime\", \"failures\"]]\ny_test = test_df[\"G3\"]\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)"
  },
  {
    "objectID": "blog-post.html#step-2-baseline-model-with-dummy-regressor",
    "href": "blog-post.html#step-2-baseline-model-with-dummy-regressor",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "Step 2: Baseline Model with Dummy Regressor",
    "text": "Step 2: Baseline Model with Dummy Regressor\nWe begin by training a Dummy Regressor, which predicts the mean of the target variable for all samples. This serves as a baseline to evaluate whether our Ridge Regression model improves prediction accuracy.\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Train and evaluate the Dummy Regressor\ndummy_model = DummyRegressor(strategy=\"mean\")\ndummy_model.fit(X_train_scaled, y_train)\n\n# Predictions and evaluation\ny_pred_dummy = dummy_model.predict(X_test_scaled)\nmse_dummy = mean_squared_error(y_test, y_pred_dummy)\nprint(f\"Dummy Regressor MSE: {mse_dummy:.2f}\")\n\nDummy Regressor MSE: 22.71"
  },
  {
    "objectID": "blog-post.html#step-3-train-a-ridge-regression-model",
    "href": "blog-post.html#step-3-train-a-ridge-regression-model",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "Step 3: Train a Ridge Regression Model",
    "text": "Step 3: Train a Ridge Regression Model\nNext, we train a Ridge Regression model and tune its hyperparameter, alpha using GridSearchCV. This ensures the model generalizes well to unseen data by evaluating multiple hyperparameter values through cross-validation.\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\n# Perform Grid Search with Ridge Regression\nparams = {\"alpha\": [0.1, 1, 10, 100]}\nridge = Ridge()\ngrid_search = GridSearchCV(\n    estimator=ridge, \n    param_grid=params, \n    cv=5, \n    scoring=\"neg_mean_squared_error\"\n)\ngrid_search.fit(X_train_scaled, y_train)\n\n# Select the best model\nbest_ridge = grid_search.best_estimator_\n\nprint(f\"Best Alpha from Grid Search: {grid_search.best_params_['alpha']}\")\n\nBest Alpha from Grid Search: 10"
  },
  {
    "objectID": "blog-post.html#step-4-evaluate-ridge-regression-performance",
    "href": "blog-post.html#step-4-evaluate-ridge-regression-performance",
    "title": "Predicting Grades and Simplifying Machine Learning Concepts",
    "section": "Step 4: Evaluate Ridge Regression Performance",
    "text": "Step 4: Evaluate Ridge Regression Performance\nFinally, we evaluate the Ridge Regression model using the test set. The following metrics are used:\n\nMean Squared Error (MSE): On average, the squared error between the predicted and actual grades is approximately 22.26.\nRoot Mean Squared Error (RMSE): On average, the model’s predictions are off by about 4.72 grade points.\nMean Absolute Error (MAE): On average, the model’s predictions are off by 3.50 grade points, without emphasizing larger errors as much as MSE or RMSE.\n\n\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n# Predictions\ny_pred_ridge = best_ridge.predict(X_test_scaled)\n\n# Calulate MSE\nmse_ridge = mean_squared_error(y_test, y_pred_ridge)\n\n# Calculate RMSE\nrmse_ridge = np.sqrt(mse_ridge)\n\n# Calculate MAE\nmae_ridge = mean_absolute_error(y_test, y_pred_ridge)\n\nprint(f\"Ridge Regression MSE: {mse_ridge:.2f}\")\nprint(f\"Ridge Regression RMSE: {rmse_ridge:.2f}\")\nprint(f\"Ridge Regression MAE: {mae_ridge:.2f}\")\n\nRidge Regression MSE: 22.26\nRidge Regression RMSE: 4.72\nRidge Regression MAE: 3.50"
  }
]